{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAa1M1iQWpcI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slPaOVZobUJq",
        "outputId": "a80a8c17-21cd-4f44-9ddb-c749f87338c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "file_path = '/content/Spotify Million Song Dataset_exported.csv'\n",
        "\n",
        "\n",
        "try:\n",
        "    spotify_df = pd.read_csv(file_path, encoding='utf-8')\n",
        "    print(\"Dataset loaded successfully!\")\n",
        "except pd.errors.ParserError as e:\n",
        "    print(\"Error loading dataset:\", e)\n",
        "    print(\"Attempting to read with error_bad_lines=False...\")\n",
        "\n",
        "    try:\n",
        "        spotify_df = pd.read_csv(file_path, error_bad_lines=False)\n",
        "        print(\"Dataset loaded successfully with error_bad_lines=False!\")\n",
        "    except pd.errors.ParserError as e:\n",
        "        print(\"Error loading dataset even with error_bad_lines=False:\", e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVZNka9SeFrV",
        "outputId": "6a2129ea-b169-4b21-f5fc-01b1a6e5934a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  artist                   song                                        link  \\\n",
            "0   ABBA  Ahe's My Kind Of Girl  /a/abba/ahes+my+kind+of+girl_20598417.html   \n",
            "1   ABBA       Andante, Andante       /a/abba/andante+andante_20002708.html   \n",
            "2   ABBA         As Good As New        /a/abba/as+good+as+new_20003033.html   \n",
            "3   ABBA                   Bang                  /a/abba/bang_20598415.html   \n",
            "4   ABBA       Bang-A-Boomerang      /a/abba/bang+a+boomerang_20002668.html   \n",
            "\n",
            "                                                text  \n",
            "0  Look at her face, it's a wonderful face  \\nAnd...  \n",
            "1  Take it easy with me, please  \\nTouch me gentl...  \n",
            "2  I'll never know why I had to go  \\nWhy I had t...  \n",
            "3  Making somebody happy is a question of give an...  \n",
            "4  Making somebody happy is a question of give an...  \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 57650 entries, 0 to 57649\n",
            "Data columns (total 4 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   artist  57650 non-null  object\n",
            " 1   song    57650 non-null  object\n",
            " 2   link    57650 non-null  object\n",
            " 3   text    57650 non-null  object\n",
            "dtypes: object(4)\n",
            "memory usage: 1.8+ MB\n",
            "None\n",
            "              artist                                    song  \\\n",
            "count          57650                                   57650   \n",
            "unique           643                                   44824   \n",
            "top     Donna Summer  Have Yourself A Merry Little Christmas   \n",
            "freq             191                                      35   \n",
            "\n",
            "                                              link  \\\n",
            "count                                        57650   \n",
            "unique                                       57650   \n",
            "top     /a/abba/ahes+my+kind+of+girl_20598417.html   \n",
            "freq                                             1   \n",
            "\n",
            "                                                     text  \n",
            "count                                               57650  \n",
            "unique                                              57494  \n",
            "top     I just came back from a lovely trip along the ...  \n",
            "freq                                                    6  \n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(spotify_df.head())\n",
        "\n",
        "\n",
        "print(spotify_df.info())\n",
        "\n",
        "\n",
        "print(spotify_df.describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w52lL2a4eb7u"
      },
      "outputs": [],
      "source": [
        "\n",
        "spotify_df.dropna(inplace=True)\n",
        "\n",
        "\n",
        "spotify_df.drop_duplicates(inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0trDyUPf7ea",
        "outputId": "7df7ac71-4679-4883-805c-dfba14d5c5e4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of rows before tokenization: 57650\n",
            "  artist                   song                                        link  \\\n",
            "0   ABBA  Ahe's My Kind Of Girl  /a/abba/ahes+my+kind+of+girl_20598417.html   \n",
            "1   ABBA       Andante, Andante       /a/abba/andante+andante_20002708.html   \n",
            "2   ABBA         As Good As New        /a/abba/as+good+as+new_20003033.html   \n",
            "3   ABBA                   Bang                  /a/abba/bang_20598415.html   \n",
            "4   ABBA       Bang-A-Boomerang      /a/abba/bang+a+boomerang_20002668.html   \n",
            "\n",
            "                                                text tokenized_text  \n",
            "0  Look at her face, it's a wonderful face  \\nAnd...         [ABBA]  \n",
            "1  Take it easy with me, please  \\nTouch me gentl...         [ABBA]  \n",
            "2  I'll never know why I had to go  \\nWhy I had t...         [ABBA]  \n",
            "3  Making somebody happy is a question of give an...         [ABBA]  \n",
            "4  Making somebody happy is a question of give an...         [ABBA]  \n",
            "Number of rows after tokenization: 57650\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "print(\"Number of rows before tokenization:\", len(spotify_df))\n",
        "\n",
        "\n",
        "spotify_df['tokenized_text'] = spotify_df.apply(lambda row: word_tokenize(row['artist']), axis=1)\n",
        "\n",
        "\n",
        "print(spotify_df.head())\n",
        "\n",
        "\n",
        "print(\"Number of rows after tokenization:\", len(spotify_df))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dETULbaDhXxL",
        "outputId": "34d373e1-89a8-4640-d598-dd24ca6be344"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "\n",
        "def clean_text(tokens):\n",
        "    tokens = [word.lower() for word in tokens if word not in string.punctuation]\n",
        "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
        "    return tokens\n",
        "\n",
        "\n",
        "spotify_df['cleaned_text'] = spotify_df['tokenized_text'].apply(clean_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5AFytn4iTsJ",
        "outputId": "3f6ec632-fdee-43f5-8f71-74036d1ad132"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  artist                   song                                        link  \\\n",
            "0   ABBA  Ahe's My Kind Of Girl  /a/abba/ahes+my+kind+of+girl_20598417.html   \n",
            "1   ABBA       Andante, Andante       /a/abba/andante+andante_20002708.html   \n",
            "2   ABBA         As Good As New        /a/abba/as+good+as+new_20003033.html   \n",
            "3   ABBA                   Bang                  /a/abba/bang_20598415.html   \n",
            "4   ABBA       Bang-A-Boomerang      /a/abba/bang+a+boomerang_20002668.html   \n",
            "\n",
            "                                                text tokenized_text  \\\n",
            "0  Look at her face, it's a wonderful face  \\nAnd...         [ABBA]   \n",
            "1  Take it easy with me, please  \\nTouch me gentl...         [ABBA]   \n",
            "2  I'll never know why I had to go  \\nWhy I had t...         [ABBA]   \n",
            "3  Making somebody happy is a question of give an...         [ABBA]   \n",
            "4  Making somebody happy is a question of give an...         [ABBA]   \n",
            "\n",
            "  cleaned_text  \n",
            "0       [abba]  \n",
            "1       [abba]  \n",
            "2       [abba]  \n",
            "3       [abba]  \n",
            "4       [abba]  \n"
          ]
        }
      ],
      "source": [
        "print(spotify_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnppAREXijt9"
      },
      "outputs": [],
      "source": [
        "\n",
        "spotify_df.to_csv('spotify_dataset.csv', index=False)\n",
        "\n",
        "\n",
        "from google.colab import files\n",
        "files.download('spotify_dataset.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVvK57pIt9C-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7kMDGAiyT3b"
      },
      "outputs": [],
      "source": [
        "text_data = ' '.join(spotify_df['artist'].astype(str))\n",
        "words = text_data.split()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtlTm1UquJfE"
      },
      "outputs": [],
      "source": [
        "word_frequency = {}\n",
        "for word in words:\n",
        "  word_frequency[word] = word_frequency.get(word, 0) + 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4WflyIpy36-"
      },
      "outputs": [],
      "source": [
        "most_frequent_word = max(word_frequency, key=word_frequency.get)\n",
        "frequency_of_most_frequent_word = word_frequency[most_frequent_word]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIZvh-tey975",
        "outputId": "fe96720d-b505-41db-9b76-5ffd664d0f80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The most frequent word in the 'artist' column is 'John' with a frequency of 968.\n"
          ]
        }
      ],
      "source": [
        "print(f\"The most frequent word in the 'artist' column is '{most_frequent_word}' with a frequency of {frequency_of_most_frequent_word}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwyROT-pzUmB",
        "outputId": "f7231b9e-854a-4cb3-f36c-db112c205d43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The most frequent word in the 'song' column is 'The' with a frequency of 7933.\n"
          ]
        }
      ],
      "source": [
        "text_data = ' '.join(spotify_df['song'].astype(str))\n",
        "words1 = text_data.split()\n",
        "word_frequency1 = {}\n",
        "for word1 in words1:\n",
        "  word_frequency1[word1] = word_frequency1.get(word1, 0) + 1\n",
        "most_frequent_word1 = max(word_frequency1, key=word_frequency1.get)\n",
        "frequency_of_most_frequent_word1 = word_frequency1[most_frequent_word1]\n",
        "print(f\"The most frequent word in the 'song' column is '{most_frequent_word1}' with a frequency of {frequency_of_most_frequent_word1}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XRITF7v0XBj",
        "outputId": "7dff95ce-9346-4714-c501-7c801306e6b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The most frequent word in the 'text' column is 'the' with a frequency of 446872.\n"
          ]
        }
      ],
      "source": [
        "text_data = ' '.join(spotify_df['text'].astype(str))\n",
        "words2 = text_data.split()\n",
        "word_frequency2 = {}\n",
        "for word2 in words2:\n",
        "  word_frequency2[word2] = word_frequency2.get(word2, 0) + 1\n",
        "most_frequent_word2 = max(word_frequency2, key=word_frequency2.get)\n",
        "frequency_of_most_frequent_word2 = word_frequency2[most_frequent_word2]\n",
        "print(f\"The most frequent word in the 'text' column is '{most_frequent_word2}' with a frequency of {frequency_of_most_frequent_word2}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HJdmn192hUp",
        "outputId": "9c016946-bd58-4684-fc05-d017d6bf83ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Empty DataFrame\n",
            "Columns: []\n",
            "Index: []\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-21-17e700364d2e>:1: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
            "  correlation_matrix = spotify_df.corr()\n"
          ]
        }
      ],
      "source": [
        "correlation_matrix = spotify_df.corr()\n",
        "print(correlation_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gn-meAxQ4Vnq",
        "outputId": "ef407e45-f9f5-46cd-f47a-c77a260ad6bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Largest element in column 'artist': Joseph And The Amazing Technicolor Dreamcoat, Size: 44\n",
            "Largest element in column 'song': A Simple Desultory Philippic (Or How I Was Robert McNamara'd Into Submission), Size: 77\n",
            "Largest element in column 'link': /p/paul+simon/a+simple+desultory+philippic+or+how+i+was+robert+mcnamarad+into+submission_20261112.html, Size: 102\n",
            "Largest element in column 'text': Hello (Hello) allow me to introduce myself (myself)  \n",
            "My name is Shaaady  \n",
            "So nice to meet you (So nice to meet you)  \n",
            "It's been a long time (long time) sorry I've been away so long (So long)  \n",
            "My name is Shaaady  \n",
            "I never meant to leave you (Never meant to leave you)  \n",
            "  \n",
            "Yea  \n",
            "  \n",
            "You see that chick in the gym checkin' me out?  \n",
            "Any second I'm 'bout to stick her neck in my mouth  \n",
            "I lose a pill and I'm wrecklessly wreckin' the house  \n",
            "That was supposed to be breakfast where the heck is it now?  \n",
            "Here's the necklace I lost, right next to Stephanie's blouse  \n",
            "Man I should check to see if my mom left any out  \n",
            "Nope, guess I'll re-route  \n",
            "Maybe somewhere in the depths of your couch  \n",
            "Oh! Jackpot! Yea, 'open sesame' mouth  \n",
            "Down the hatchet, the feelin' you can't match it  \n",
            "I rap-tap-tap on your door with a damn ratchet  \n",
            "I tac-tac-tacin' the whore with the damn hatchet  \n",
            "I nap-sack-pack with like forty some Xanaxes  \n",
            "Shorty come back, I'm tryin' to score me some lap dancers  \n",
            "I'm 'bout to relapse so baby pour me some Jack Daniels  \n",
            "Formula forty-four and forty's with mad capsules  \n",
            "The bad apple spoils the bunch and back at-ya  \n",
            "  \n",
            "Well I don't mean any harm all I wanted to do is just say hello  \n",
            "And are you menstruating baby my little friend's waitin' to say hello  \n",
            "The way your titties are wiggling and your booty shakin' like Jell-O  \n",
            "Girl I don't mean any harm all I wanted to do is just say hello  \n",
            "  \n",
            "Yea  \n",
            "  \n",
            "My equilibrium's off, must be the Lithium  \n",
            "I don't need to buy any drugs, man people give me 'em  \n",
            "It just becomes everyday extracurricular  \n",
            "No reason in particular, it was strictly fun  \n",
            "A fifth of rum and two bottles of one fifty one  \n",
            "Fifty one people asleep in my damn livin' room  \n",
            "'Scuse me hun, but what was your name, Vivian?  \n",
            "I woke up next to you naked and uh, did we umm?  \n",
            "Of course we did, but didn't I strap Jimmy hun?  \n",
            "I'm looking for the torn wrapper but there doesn't seem to be one  \n",
            "No offense baby girl I don't mean any harm  \n",
            "But disease is something I'm trying to keep my penis free from  \n",
            "I find the package and I'm cool, I immediately run  \n",
            "like Speedy Gonzales to see if I see anyone  \n",
            "Who might have a couple of threes, I'm fiendin' for some  \n",
            "My head is poundin' to the beat of the drum  \n",
            "  \n",
            "Umm  \n",
            "  \n",
            "Well I don't mean any harm all I wanted to do is just say hello  \n",
            "And do you happen to have anything on you to make my mood mellow?  \n",
            "So I'm hopin' that some are pink some are blue and some are just yellow  \n",
            "Girl I don't mean any harm all I wanted to do is just say hello  \n",
            "  \n",
            "Oh, Those were the days they certainly were  \n",
            "It's hurtin' me to know that I'll be closin' the curtain for good  \n",
            "(Word-word, chi-word-word, chi-word-chi-chi-word)  \n",
            "I second that and a third, rush to emergency surgery  \n",
            "To try to flush me because of the drugs that he purchased  \n",
            "He no longer gettin' the three hundred bucks for these Perk-idans  \n",
            "Buster's gettin' to be where he lustfully surged it in  \n",
            "Pain is hitting his knee and his muscles 'be hurtin' him'  \n",
            "Childishly on the phone tryin' to rustle up  \n",
            "Muscle relaxers for his back and a couple of Paxils  \n",
            "Now he's doubling backwards and he's stumblin' back  \n",
            "Slipped and fell hit his backbone heard somethin' go crack  \n",
            "Now he's up in the bathroom like he's bustin' a nap  \n",
            "Almost ended it that soon because of the fact  \n",
            "I'm just bustin' my own chops while I'm bustin' a rap  \n",
            "Resuscitated and re-juiced so fuck it' I'm back  \n",
            "  \n",
            "Well I don't mean any harm all I wanted to do is just say hello  \n",
            "And do you happen to have anything on you to make my mood mellow?  \n",
            "So I'm hopin' that some are pink some are blue and some are just yellow  \n",
            "Girl I don't mean any harm all I wanted to do is just say hello  \n",
            "  \n",
            "So I'm sayin  \n",
            "Hello (Hello) allow me to introduce myself (myself)  \n",
            "My name is Shaaady  \n",
            "So nice to meet you (So nice to meet you)  \n",
            "It's been a long time (long time) sorry I've been away so long (So long)  \n",
            "My name is Shaaady  \n",
            "I never meant to leave you (Never meant to leave you)\n",
            "\n",
            ", Size: 3997\n",
            "Largest element in column 'tokenized_text': ['Joseph', 'And', 'The', 'Amazing', 'Technicolor', 'Dreamcoat'], Size: 63\n",
            "Largest element in column 'cleaned_text': ['joseph', 'amazing', 'technicolor', 'dreamcoat'], Size: 49\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "max_elements = {}\n",
        "for column in spotify_df.columns:\n",
        "    max_element = spotify_df[column].apply(lambda x: len(str(x))).max()\n",
        "    max_elements[column] = (spotify_df[column][spotify_df[column].apply(lambda x: len(str(x))) == max_element].values[0], max_element)\n",
        "\n",
        "\n",
        "for column, (element, size) in max_elements.items():\n",
        "    print(f\"Largest element in column '{column}': {element}, Size: {size}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
